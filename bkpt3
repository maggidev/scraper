import asyncio
import json
import re
import warnings
from dataclasses import dataclass, field, asdict
from typing import List, Optional
from curl_cffi.requests import AsyncSession
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning

# Silencia avisos de parser para manter o console limpo
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

@dataclass
class Video:
    quality: str
    url: str

@dataclass
class Episode:
    number: str
    url: str
    thumb: str = "" 
    videos: List[Video] = field(default_factory=list)

@dataclass
class AnimeData:
    title: str
    slug: str
    url: str
    thumbnail: str 
    description: str
    genres: List[str]
    status: str
    year: str
    episodes: List[Episode] = field(default_factory=list)

class AnimeFireAPI:
    def __init__(self):
        self.base_url = "https://animefire.io"
        # Impersonate simula um navegador real para evitar bloqueios Cloudflare
        self.session = AsyncSession(impersonate="chrome110")
        self.session.headers.update({"Referer": self.base_url})
        # Sem√°foro para evitar sobrecarga no servidor e erro 429
        self.semaphore = asyncio.Semaphore(15) 

    def _get_info_text(self, soup, label):
        """
        Busca metadados (Status, Ano) procurando pelo texto dentro de tags <b>.
        Ajustado para a estrutura real do site.
        """
        for info in soup.select(".animeInfo"):
            b_tag = info.select_one("b")
            if b_tag and label.lower() in b_tag.text.lower():
                span = info.select_one("span")
                return span.text.strip() if span else "N/A"
        return "N/A"

    async def get_all_links_from_sitemap(self) -> List[str]:
        print("üì• Acessando sitemap para buscar lista de animes...")
        try:
            resp = await self.session.get(f"{self.base_url}/sitemap.xml")
            soup = BeautifulSoup(resp.text, features="xml")
            # Filtra apenas links de listagem de epis√≥dios
            links = [loc.text for loc in soup.find_all("loc") if "-todos-os-episodios" in loc.text]
            return sorted(list(set(links)))
        except Exception as e:
            print(f"‚ùå Erro ao baixar sitemap: {e}")
            return []

    async def get_video_links(self, ep_name: str, ep_url: str, anime_slug: str) -> Episode:
        """Extrai links de v√≠deo e gera thumbnail do epis√≥dio via endpoint fixo."""
        async with self.semaphore:
            try:
                # Extrai o n√∫mero do epis√≥dio da URL para compor a thumb (ex: .../12 -> 12)
                ep_number = ep_url.split("/")[-1]
                
                ep_thumb = f"https://animefire.io/img/video/{anime_slug}/{ep_number}.webp"

                resp = await self.session.get(ep_url, timeout=15)
                soup = BeautifulSoup(resp.text, "html.parser")
                videos = []

                # Tenta pegar v√≠deos via data-video-src (API interna do site)
                video_tag = soup.select_one("video#my-video")
                if video_tag and video_tag.get("data-video-src"):
                    api_resp = await self.session.get(video_tag["data-video-src"])
                    for item in api_resp.json().get("data", []):
                        videos.append(Video(quality=item.get("label"), url=item.get("src")))

                # Fallback para Iframe (players externos)
                if not videos:
                    iframe = soup.select_one("div#div_video iframe")
                    if iframe:
                        iframe_resp = await self.session.get(iframe.get("src"))
                        m = re.search(r'play_url"\s*:\s*"([^"]+)', iframe_resp.text)
                        if m: videos.append(Video(quality="SD/HD", url=m.group(1)))
                
                return Episode(number=ep_name, url=ep_url, thumb=ep_thumb, videos=videos)
            except Exception:
                return Episode(number=ep_name, url=ep_url, thumb=ep_thumb, videos=[])

    async def scrape_full_anime(self, url: str) -> Optional[AnimeData]:
        """Faz o scrape completo da p√°gina do anime."""
        async with self.semaphore:
            try:
                resp = await self.session.get(url, timeout=20)
                soup = BeautifulSoup(resp.text, "html.parser")
                
                title_tag = soup.find("h1")
                if not title_tag: return None
                
                # Define o slug base para constru√ß√£o de imagens
                anime_slug = url.split("/")[-1].replace("-todos-os-episodios", "")
                
                # L√≥gica din√¢mica de capa: {slug}-large.webp
                cover_url = f"https://animefire.io/img/animes/{anime_slug}-large.webp"

                anime = AnimeData(
                    title=title_tag.text.strip(),
                    slug=anime_slug,
                    url=url,
                    thumbnail=cover_url,
                    description=soup.select_one("div.divSinopse").text.strip() if soup.select_one("div.divSinopse") else "",
                    genres=[a.text.strip() for a in soup.select("a.spanGeneros")],
                    status=self._get_info_text(soup, "Status"),
                    year=self._get_info_text(soup, "Ano")
                )

                # Coleta todos os links de epis√≥dios dispon√≠veis
                ep_elements = soup.select("div.div_video_list > a")
                if ep_elements:
                    tasks = [self.get_video_links(a.text.strip(), a["href"], anime_slug) for a in ep_elements]
                    anime.episodes = await asyncio.gather(*tasks)

                return anime
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao processar anime {url}: {e}")
                return None

    async def run_automation(self):
        """Gerencia o processo de extra√ß√£o e salvamento incremental."""
        links = await self.get_all_links_from_sitemap()
        total_sitemap = len(links)
        
        all_data = []
        processed_slugs = set()
        
        # Carrega progresso anterior para evitar retrabalho
        try:
            with open("animes_full_db.json", "r", encoding="utf-8") as f:
                all_data = json.load(f)
                processed_slugs = {anime['slug'] for anime in all_data}
                print(f"üì¶ Checkpoint: {len(processed_slugs)} animes j√° carregados.")
        except (FileNotFoundError, json.JSONDecodeError):
            print("üÜï Criando novo banco de dados...")

        # Filtra apenas links que ainda n√£o foram processados
        links_to_process = [
            l for l in links 
            if l.split("/")[-1].replace("-todos-os-episodios", "") not in processed_slugs
        ]
        
        if not links_to_process:
            print("‚úÖ O banco de dados j√° est√° totalmente atualizado.")
            return

        print(f"üöÄ Iniciando extra√ß√£o de {len(links_to_process)} novos itens...")

        batch_size = 10 
        for i in range(0, len(links_to_process), batch_size):
            batch = links_to_process[i : i + batch_size]
            tasks = [self.scrape_full_anime(link) for link in batch]
            results = await asyncio.gather(*tasks)
            
            # Adiciona resultados v√°lidos √† lista e salva no JSON
            valid_results = [asdict(r) for r in results if r]
            all_data.extend(valid_results)
            
            with open("animes_full_db.json", "w", encoding="utf-8") as f:
                json.dump(all_data, f, indent=4, ensure_ascii=False)
            
            print(f"‚è≥ Salvo: {len(all_data)} / {total_sitemap} animes totais.")

async def main():
    api = AnimeFireAPI()
    await api.run_automation()

if __name__ == "__main__":
    asyncio.run(main())